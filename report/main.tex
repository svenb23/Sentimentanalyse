\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{float}

\usepackage[scaled]{helvet}
\renewcommand{\familydefault}{\sfdefault}

\usepackage[a4paper, top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

\usepackage{setspace}
\setstretch{1.5}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

\usepackage{microtype}
\sloppy
\hyphenpenalty=1000
\tolerance=3000

\renewcommand{\footnotesize}{\fontsize{10}{12}\selectfont}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\usepackage{titlesec}
\titleformat{\section}{\normalfont\fontsize{12}{14}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\fontsize{12}{14}\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\fontsize{12}{14}\bfseries}{\thesubsubsection}{1em}{}

\usepackage[
  colorlinks=true,
  linkcolor=black,
  citecolor=blue,
  filecolor=black,
  urlcolor=blue
]{hyperref}
\usepackage[capitalise,nameinlink]{cleveref}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[C]{\thepage}

\usepackage[backend=biber, style=apa]{biblatex}
\addbibresource{references.bib}

\usepackage{titling}

\usepackage{acronym}

\usepackage{caption}
\usepackage{threeparttable}
\captionsetup[table]{
    font=small,
    skip=10pt,
    labelfont=bf
}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}

\begin{titlepage}
    \thispagestyle{empty}
    \centering
    \vspace*{5cm}
    {\Huge\bfseries Projekt: NLP DLBAIPNLP01\_D \par}
    \vspace{1cm}
    {\Large Projektbericht \par}
    \vspace{0.5cm}
    {\large Studiengang: Angewandte Künstliche Intelligenz \par}
    \vspace{0.5cm}
    {\large Sven Behrens \par}
    \vspace{0.5cm}
    {\large Matrikelnummer: 42303511 \par}
    \vspace{0.5cm}
    {\large Tutor: Prof. Dr. Maja Popovic \par}
    \vspace{0.5cm}
    {\large \today \par}
\end{titlepage}

\pagenumbering{Roman}
\setcounter{page}{1}

\tableofcontents
\newpage

\listoffigures
\addcontentsline{toc}{section}{Abbildungsverzeichnis}
\newpage

\listoftables
\addcontentsline{toc}{section}{Tabellenverzeichnis}
\newpage

\section*{Abkürzungsverzeichnis}
\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
\begin{acronym}[DistilBERT]
    \acro{API}{Application Programming Interface}
\end{acronym}
\newpage

\pagenumbering{arabic}
\setcounter{page}{1}

\section{Einleitung}
Durch die zunehmende Digitalisierung werden Benutzerbewertungen sowohl zur Beurteilung
von Produkten als auch in sozialen Medien immer wichtiger, sowohl für Kunden, um einen
schnellen Überblick zu bekommen, als auch für Unternehmen, um Feedback systematisch auszuwerten.
So werden täglich Millionen von Nutzermeinungen erzeugt, deren manuelle Analyse
längst nicht mehr praktikabel ist. Die automatisierte
Sentimentanalyse von Produktrezensionen stellt dabei eine zentrale Herausforderung
im Bereich des Natural Language Processing dar, deren Bewältigung für Unternehmen
maßgeblich zur Produktverbesserung und Kundenzufriedenheitsanalyse beiträgt.
Vor diesem Hintergrund wurde im Rahmen des Moduls „Projekt: NLP" an der IU
Internationalen Hochschule ein umfassendes System zur Sentimentanalyse von
Amazon-Produktbewertungen entwickelt, das sowohl klassische
Machine-Learning-Verfahren als auch moderne Transformer-Architekturen systematisch
vergleicht.

Das primäre Projektziel bestand in der Entwicklung und dem Vergleich verschiedener
Klassifikationsansätze zur automatischen Sentiment-Erkennung aus englischsprachigen
Produktrezensionen. Die zentrale Forschungsfrage konzentrierte sich darauf, wie sich
traditionelle Machine-Learning-Verfahren, mit Modellen wie Logistic Regression gegenüber
modernen vortrainierten Sprachmodellen wie DistilBERT hinsichtlich
Klassifikationsgenauigkeit und Praxistauglichkeit verhalten.

Die Datenbasis bildeten Amazon-Produktrezensionen aus drei Kategorien, Automotive,
Pet Supplies und Video Games, mit insgesamt 150.000 Datensätzen, die stratifiziert
in Trainings- (70\%), Validierungs- (15\%) und Testdaten (15\%) aufgeteilt wurden.
Jede Rezension umfasst Titel, Text und eine Sternebewertung von 1 bis 5, wobei
sowohl eine 5-Klassen-Klassifikation als auch eine aggregierte 
3-Klassen-Sentimentanalyse (negativ, neutral, positiv) untersucht wurden.

Die methodische Vorgehensweise gliederte sich in mehrere aufeinander aufbauende
Phasen. Nach einer initialen Datenaufbereitung mit Textbereinigung,
Lemmatisierung und Stopwort-Entfernung erfolgte zunächst die Implementierung
klassischer Modelle unter Verwendung von TF-IDF-Vektorisierung mit
bis zu 20.000 Features. Hierbei wurden Logistic Regression, Naive Bayes, SVM,
Random Forest und Gradient Boosting systematisch evaluiert. Parallel dazu wurde 
ein Fine-Tuning des DistilBERT-Modells auf dem vollständigen Trainingsdatensatz 
durchgeführt sowie ein vortrainiertes BERT-Modell für Sentimentanalyse im 
Zero-Shot-Modus getestet.

Der vorliegende Projektbericht dokumentiert systematisch den gesamten
Entwicklungsprozess. Nach dieser Einleitung folgt die detaillierte Beschreibung 
der Methodik, einschließlich der Datenaufbereitung sowie der 
Modellauswahl und -konfiguration. Anschließend werden die Trainingsabläufe und 
deren Ergebnisse ausführlich dargestellt, wobei das beste Modell (DistilBERT)
eine Testgenauigkeit von 80,91\% erreichte, während die klassische Logistic
Regression mit 76,66\% konkurrenzfähige Referenzleistung erzielte.
Abschließend werden die gewonnenen Erkenntnisse in einem Fazit zusammengefasst
und kritisch reflektiert.


\section{Hauptteil}
\subsection{Hardwareauswahl}
Zu Projektbeginn standen drei Alternativen zur Verfügung: ein MacBook Pro mit Apple-M2-Chip,
ein Windows-PC mit NVIDIA RTX 3060 sowie verschiedene Cloud-Computing-Anbieter.

Für das Training der klassischen Machine-Learning-Modelle wie Logistic Regression,
Naive Bayes und SVM war sowohl das MacBook als auch der Windows-PC ausreichend,
da diese Verfahren primär CPU-basiert arbeiten. Für das Fine-Tuning der BERT-Modelle
schied das MacBook jedoch aufgrund der fehlenden CUDA-Unterstützung aus.

Das Training wurde daher auf dem Windows-11-Computer (Intel i7-12700K, 32 GB DDR5,
NVIDIA RTX 3060 mit 12 GB VRAM) durchgeführt. Da selbst das längste Training
(DistilBERT auf 105.000 Samples) mit etwa zwei Tagen in einem akzeptablen Rahmen blieb,
waren zusätzliche Cloud-Lösungen nicht erforderlich. 

\subsection{Projektumgebung einrichten}
Zu Beginn des Projekts wurde ein GitHub-Repository angelegt. Obwohl nur eine Person am Projekt
arbeitet, ermöglicht GitHub eine nachvollziehbare Versionsverwaltung und ein einfaches Zurücksetzen
auf frühere Stände. Anschließend wurde eine grundlegende Verzeichnisstruktur erstellt und mit
\texttt{venv} eine virtuelle Umgebung eingerichtet, in der zentrale Bibliotheken installiert wurden.

\subsection{Datenbeschaffung}
Als Datenquelle diente der Amazon Reviews 2023 Datensatz \cite{hou2024bridging}, der über 500 Millionen
Produktbewertungen aus verschiedenen Kategorien umfasst und für akademische Forschungszwecke frei verfügbar ist.

Bei der Auswahl der Kategorien wurden zunächst kleinere Kategorien wie Gift Cards in Betracht gezogen.
Diese erwiesen sich jedoch als ungeeignet, da die zugehörigen Rezensionen häufig nur aus sehr kurzen Texten wie
„gut" oder „schnelle Lieferung" bestanden, die für eine aussagekräftige Sentimentanalyse nicht ausreichend sind.

Stattdessen wurden die Kategorien Automotive, Pet Supplies und Video Games ausgewählt. Diese Kategorien zeichnen
sich durch ausführlichere Rezensionen aus, in denen Nutzer ihre Erfahrungen detailliert beschreiben. Zudem repräsentieren
sie unterschiedliche Produkttypen, wodurch die Generalisierungsfähigkeit der trainierten Modelle besser evaluiert werden kann.

Aus jeder Kategorie wurden jeweils 50.000 Rezensionen entnommen, sodass ein balancierter Gesamtdatensatz von 150.000
Datensätzen entstand.

\subsection{Datenanalyse}
\subsubsection{Datensatzstruktur}
In einem ersten Schritt wurden die Rohdaten untersucht und ihre Struktur analysiert.
Die Rezensionen liegen im JSONL-Format vor, wobei jede Zeile einen JSON-Datensatz
repräsentiert. \Cref{tab:json_fields} zeigt die verfügbaren Felder und ihre Bedeutung.

\begin{table}[H]
\centering
\caption{Struktur der Amazon-Rezensionsdaten im JSONL-Format}
\label{tab:json_fields}
\begin{tabular}{llp{5.5cm}}
\toprule
\textbf{Feld} & \textbf{Beschreibung} & \textbf{Beispiel} \\
\midrule
rating & Sternebewertung (1.0--5.0) & 5.0 \\
title & Titel der Rezension & „Great product!'' \\
text & Ausführlicher Rezensionstext &  „Item came as described...'' \\
images & Angehängte Bilder & {[}{]} \\
asin & Amazon Produkt-ID & B01LZA8SGZ \\
parent\_asin & Übergeordnete Produkt-ID & B0BV88374L \\
user\_id & Anonymisierte Benutzer-ID & AGXVBIUFLFGMV... \\
timestamp & Unix-Zeitstempel & 1513092936205 \\
helpful\_vote & Anzahl hilfreicher Stimmen & 0 \\
verified\_purchase & Verifizierter Kauf & true \\
\bottomrule
\end{tabular}
\end{table}

Die Felder \texttt{title} und \texttt{text} wurden zu einem
zusammenhängenden Eingabetext (X) kombiniert, während das Feld \texttt{rating} als Zielklasse (y)
mit Werten von 1 bis 5 Sternen dient. Zusätzlich wurde die Produktkategorie gespeichert,
um kategoriespezifische Evaluationen zu ermöglichen.

Anschließend erfolgte die Aufteilung der Daten in 70\% Trainings-, 15\% Validierungs- und 15\% Testdaten.

\subsection{Vorverarbeitungs-Pipeline}
Die folgenden Ausführungen zur Textvorverarbeitung und Feature-Extraktion orientieren sich
an den Grundlagen aus \textcite[S.~62--78]{schaaff2025nlp}.

Die Textvorverarbeitung bildet einen entscheidenden Schritt zur Vorbereitung der Rohtexte
für die maschinelle Verarbeitung. Ziel ist es, Rauschen zu reduzieren und die relevanten
sprachlichen Merkmale zu extrahieren.

\subsubsection{Textbereinigung und Normalisierung}
Die implementierte Pipeline verarbeitet jeden Rezensionstext in mehreren aufeinander
aufbauenden Schritten:

\begin{enumerate}
    \item \textbf{Textkombination}: Zusammenführung von \texttt{title} und \texttt{text} zu einem Eingabestring
    \item \textbf{Kleinschreibung}: Konvertierung aller Zeichen zu Kleinbuchstaben zur Vereinheitlichung
    \item \textbf{HTML-Bereinigung}: Entfernung von HTML-Tags mittels regulärer Ausdrücke
    \item \textbf{URL-Entfernung}: Filterung von Weblinks, die keine semantische Relevanz besitzen
    \item \textbf{Zahlenentfernung}: Eliminierung numerischer Werte
    \item \textbf{Satzzeichenentfernung}: Entfernung aller Interpunktionszeichen
    \item \textbf{Tokenisierung}: Zerlegung des Textes in einzelne Wörter mittels NLTK
    \item \textbf{Stoppwort-Filterung}: Entfernung häufiger englischer Funktionswörter (the, is, at, ...)
    \item \textbf{Lemmatisierung}: Rückführung der Wörter auf ihre Grundform mittels WordNetLemmatizer
    \item \textbf{Längenfilterung}: Entfernung von Tokens mit weniger als 2 Zeichen
\end{enumerate}

\subsubsection{Feature-Extraktion}
Für die Transformation der vorverarbeiteten Texte in numerische Vektoren stehen verschiedene
Methoden zur Verfügung. \Cref{tab:feature_methods} vergleicht die gängigsten Ansätze.

\begin{table}[H]
\centering
\caption{Vergleich der Feature-Extraktionsmethoden}
\label{tab:feature_methods}
\begin{tabular}{lp{5cm}p{5cm}}
\toprule
\textbf{Methode} & \textbf{Vorteile} & \textbf{Nachteile} \\
\midrule
Bag of Words & Einfach, schnell, interpretierbar & Ignoriert Worthäufigkeit im Korpus, große sparse Matrizen \\
TF-IDF & Gewichtet wichtige Wörter höher, reduziert Rauschen, bewährt für Klassifikation & Ignoriert Wortkontext und -reihenfolge \\
Word2Vec / GloVe & Erfasst semantische Ähnlichkeit, dichte Vektoren & Benötigt viele Trainingsdaten, schwerer interpretierbar \\
\bottomrule
\end{tabular}
\end{table}

Für die klassischen Machine-Learning-Modelle wurde \textbf{TF-IDF} (Term Frequency-Inverse
Document Frequency) gewählt. Diese Entscheidung basiert auf mehreren Faktoren: TF-IDF
ist für Sentiment-Klassifikation bewährt, ermöglicht schnelles Training ohne GPU und
bietet Interpretierbarkeit, es lässt sich nachvollziehen, welche Wörter zur Klassifikation
beitragen. 

\begin{table}[H]
\centering
\caption{Konfiguration der TF-IDF-Vektorisierung}
\label{tab:tfidf_config}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Wert} & \textbf{Beschreibung} \\
\midrule
max\_features & 10.000 & Maximale Anzahl der Features \\
ngram\_range & (1, 2) & Uni- und Bigramme \\
min\_df & 2 & Term muss in mind. 2 Dokumenten vorkommen \\
max\_df & 0,95 & Term darf in max. 95\% der Dokumente vorkommen \\
\bottomrule
\end{tabular}
\end{table}

Die Verwendung von Bigrammen ermöglicht die Erfassung von Wortpaaren wie „not good''
oder „very bad'', die für die Sentimentanalyse besonders relevant sind, da sie
Negationen und Verstärkungen berücksichtigen.


\subsection{Modellauswahl}
Um einen fundierten Vergleich zwischen klassische Machine-Learning-Verfahren und Transformer-basierte Sprachmodelle zu ermöglichen,
wurden folgende Modelle evaluiert.

\subsubsection{Klassische Machine-Learning-Modelle}
Als Baseline dienten etablierte Klassifikationsverfahren in Kombination mit TF-IDF-Vektorisierung.
Die Term Frequency-Inverse Document Frequency (TF-IDF) transformiert Texte in numerische Vektoren, wobei häufig vorkommende,
aber wenig aussagekräftige Wörter herabgewichtet werden.

Folgende Modelle wurden implementiert und evaluiert:
\begin{itemize}
    \item \textbf{Logistic Regression}: Bei der logistischen Regression wird eine S-förmige logistische Funktion (Sigmoidfunktion) an die Daten gefittet, wobei der Ausgabewert die Wahrscheinlichkeit einer bestimmten Klassenzugehörigkeit ausdrückt \parencite{iu2025supervised}.
    \item \textbf{Naive Bayes}: Dieser Klassifikator beruht auf dem Satz von Bayes und geht von der Annahme aus, dass die Verteilungen der berücksichtigten Merkmale voneinander unabhängig sind \parencite{iu2025supervised}.
    \item \textbf{Support Vector Machine (LinearSVC)}: Das Verfahren basiert auf der Einpassung einer Klassifizierungsgrenze (Hyperebene) in den hochdimensionalen Feature-Raum, wobei neue Beobachtungen je nach Position relativ zur Hyperebene klassifiziert werden \parencite{iu2025supervised}.
    \item \textbf{Random Forest}: Diese Ensemble-Methode bündelt einzelne Entscheidungsbäume durch Bagging zu einem starken Schätzer, dessen Vorhersage durch Aggregation der Einzelvorhersagen ermittelt wird \parencite{iu2025supervised}.
    \item \textbf{Gradient Boosting}: Im Gegensatz zu Random Forest erfolgt das Training der Entscheidungsbäume sequenziell, wobei jeder Schätzer die Fehler des vorherigen korrigiert \parencite{iu2025supervised}.
\end{itemize}

Die TF-IDF-Vektorisierung wurde mit bis zu 20.000 Features und Uni- sowie Bigrammen konfiguriert. Zusätzlich wurde eine Textvorverarbeitung mit Kleinschreibung, Entfernung von HTML-Tags, URLs und Sonderzeichen sowie Lemmatisierung durchgeführt.

\subsubsection{Transformer-basierte Modelle}
Für den Vergleich mit modernen Deep-Learning-Ansätzen wurden zwei BERT-basierte Modelle ausgewählt:

\textbf{DistilBERT} \cite{sanh2019distilbert} ist eine komprimierte Version des ursprünglichen BERT-Modells \cite{devlin2018bert},
die durch Knowledge Distillation erzeugt wurde. Mit 66 Millionen Parametern ist DistilBERT etwa 40\% kleiner als BERT, behält jedoch 97\%
der Sprachverständnisfähigkeiten bei. Diese Eigenschaften machen DistilBERT besonders geeignet für Projekte mit begrenzten Hardwareressourcen.

Als zweiter Ansatz wurde ein \textbf{vortrainiertes BERT-Modell für Sentimentanalyse} (nlptown/bert-base-multilingual-uncased-sentiment)
im Zero-Shot-Modus getestet. Dieses Modell wurde bereits auf Produktbewertungen trainiert und ermöglicht eine Klassifikation
ohne zusätzliches Fine-Tuning auf dem eigenen Datensatz.

\subsection{Training}
\subsubsection{Erstes Training der klassischen Modelle}
Das initiale Training erfolgte mit drei klassischen Modellen:
Logistic Regression, Multinomial Naive Bayes und LinearSVC. Das Modell mit der höchsten
Validierungsgenauigkeit wurde automatisch ausgewählt und auf dem Testdatensatz evaluiert.

Die Evaluation umfasste Accuracy, Classification Report und Confusion Matrix. Das beste Modell
wurde abschließend gespeichert.

\paragraph{Ergebnisse und Problemanalyse}
Die initiale Evaluation zeigte eine deutliche Asymmetrie in der Klassifikationsleistung.
Während 5-Sterne-Bewertungen mit einem Recall von 97\% und einer Precision von 78\% zuverlässig
erkannt wurden, offenbarten die mittleren Klassen erhebliche Schwächen.

\begin{table}[H]
\centering
\caption{Recall-Werte der mittleren Bewertungsklassen im ersten Training}
\label{tab:first_training_recall}
\begin{tabular}{lcl}
\toprule
\textbf{Bewertung} & \textbf{Recall} & \textbf{Korrekt erkannt} \\
\midrule
2 Sterne & 10\% & 24 von 230 \\
3 Sterne & 16\% & 49 von 298 \\
4 Sterne & 20\% & 105 von 528 \\
\bottomrule
\end{tabular}
\end{table}

Die Confusion Matrix in \Cref{tab:confusion_first} verdeutlicht das Kernproblem: Das Modell
tendierte dazu, Rezensionen aller Klassen als 5-Sterne-Bewertungen zu klassifizieren.
So wurden beispielsweise 163 der 1-Stern-Bewertungen fälschlicherweise als 5 Sterne eingestuft,
bei 4-Stern-Bewertungen waren es sogar 381 von 528.

\begin{table}[H]
\centering
\caption{Confusion Matrix des ersten Trainings}
\label{tab:confusion_first}
\begin{tabular}{l|ccccc}
\toprule
& \multicolumn{5}{c}{\textbf{Vorhergesagt}} \\
\textbf{Tatsächlich} & 1 & 2 & 3 & 4 & 5 \\
\midrule
1 Stern & 370 & 3 & 10 & 10 & 163 \\
2 Sterne & 82 & 24 & 23 & 19 & 82 \\
3 Sterne & 55 & 8 & 49 & 34 & 152 \\
4 Sterne & 25 & 2 & 15 & 105 & 381 \\
5 Sterne & 37 & 2 & 12 & 41 & 2796 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{}


\section{Fazit}
\subsection{Zielerreichung und Projektergebnisse}
\subsection{Kritische Reflexion und gewonnene Erkenntnisse}
\subsection{Verbesserungspotenziale und Optimierungsansätze}
\subsection{Ausblick}
\newpage

\printbibliography
\addcontentsline{toc}{section}{Literaturverzeichnis}

\newpage
\section*{Verzeichnis der Anhänge}
\addcontentsline{toc}{section}{Verzeichnis der Anhänge}

\appendix
\section*{Anhang}
\addcontentsline{toc}{section}{Anhang}

\end{document}
