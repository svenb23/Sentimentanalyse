{
  "validation": {
    "accuracy": 0.7680444444444444,
    "precision_macro": 0.5974802724811099,
    "recall_macro": 0.4758548215828041,
    "f1_macro": 0.4995410686248546,
    "precision_weighted": 0.7297128398720965,
    "recall_weighted": 0.7680444444444444,
    "f1_weighted": 0.7323634320754302,
    "confusion_matrix": [
      [
        2086,
        68,
        84,
        41,
        441
      ],
      [
        449,
        155,
        153,
        76,
        310
      ],
      [
        278,
        69,
        358,
        236,
        606
      ],
      [
        97,
        19,
        116,
        741,
        1670
      ],
      [
        162,
        11,
        65,
        268,
        13941
      ]
    ],
    "classification_report": {
      "1": {
        "precision": 0.6790364583333334,
        "recall": 0.7669117647058824,
        "f1-score": 0.7203038674033149,
        "support": 2720.0
      },
      "2": {
        "precision": 0.4813664596273292,
        "recall": 0.13560804899387577,
        "f1-score": 0.21160409556313994,
        "support": 1143.0
      },
      "3": {
        "precision": 0.46134020618556704,
        "recall": 0.23141564318034907,
        "f1-score": 0.30822212656048215,
        "support": 1547.0
      },
      "4": {
        "precision": 0.5440528634361234,
        "recall": 0.28036322360953464,
        "f1-score": 0.3700374531835206,
        "support": 2643.0
      },
      "5": {
        "precision": 0.8216053748231966,
        "recall": 0.9649754274243788,
        "f1-score": 0.887537800413815,
        "support": 14447.0
      },
      "accuracy": 0.7680444444444444,
      "macro avg": {
        "precision": 0.5974802724811099,
        "recall": 0.4758548215828041,
        "f1-score": 0.4995410686248546,
        "support": 22500.0
      },
      "weighted avg": {
        "precision": 0.7297128398720965,
        "recall": 0.7680444444444444,
        "f1-score": 0.7323634320754302,
        "support": 22500.0
      }
    }
  },
  "test": {
    "accuracy": 0.7676888888888889,
    "precision_macro": 0.6090290800405121,
    "recall_macro": 0.48011996167308224,
    "f1_macro": 0.507512335440228,
    "precision_weighted": 0.7314086902558705,
    "recall_weighted": 0.7676888888888889,
    "f1_weighted": 0.7322458702478312,
    "confusion_matrix": [
      [
        2063,
        88,
        82,
        33,
        454
      ],
      [
        449,
        179,
        135,
        60,
        321
      ],
      [
        281,
        68,
        399,
        192,
        607
      ],
      [
        86,
        15,
        109,
        695,
        1737
      ],
      [
        152,
        17,
        59,
        282,
        13937
      ]
    ],
    "classification_report": {
      "1": {
        "precision": 0.6806334543055097,
        "recall": 0.7584558823529411,
        "f1-score": 0.7174404451399756,
        "support": 2720.0
      },
      "2": {
        "precision": 0.4877384196185286,
        "recall": 0.15646853146853146,
        "f1-score": 0.2369291859695566,
        "support": 1144.0
      },
      "3": {
        "precision": 0.5089285714285714,
        "recall": 0.2579185520361991,
        "f1-score": 0.34234234234234234,
        "support": 1547.0
      },
      "4": {
        "precision": 0.5507131537242472,
        "recall": 0.2630582891748675,
        "f1-score": 0.35604508196721313,
        "support": 2642.0
      },
      "5": {
        "precision": 0.8171318011257036,
        "recall": 0.9646985533328719,
        "f1-score": 0.8848046217820525,
        "support": 14447.0
      },
      "accuracy": 0.7676888888888889,
      "macro avg": {
        "precision": 0.6090290800405121,
        "recall": 0.48011996167308224,
        "f1-score": 0.507512335440228,
        "support": 22500.0
      },
      "weighted avg": {
        "precision": 0.7314086902558705,
        "recall": 0.7676888888888889,
        "f1-score": 0.7322458702478312,
        "support": 22500.0
      }
    }
  },
  "by_category": {
    "Automotive": {
      "accuracy": 0.7853632620533936,
      "f1_macro": 0.5114227013957959,
      "count": 7529
    },
    "Video_Games": {
      "accuracy": 0.7517548596112311,
      "f1_macro": 0.516337837743259,
      "count": 7408
    },
    "Pet_Supplies": {
      "accuracy": 0.7657014412270263,
      "f1_macro": 0.4921374923251111,
      "count": 7563
    }
  },
  "experiment_id": "high_features_20251206_170827",
  "config": {
    "preprocessing": {
      "lowercase": true,
      "remove_html": true,
      "remove_urls": true,
      "remove_numbers": true,
      "remove_punctuation": true,
      "remove_stopwords": true,
      "stemming": false,
      "lemmatization": true,
      "min_token_length": 2
    },
    "features": {
      "type": "tfidf",
      "max_features": 20000,
      "ngram_range": [
        1,
        3
      ],
      "min_df": 2,
      "max_df": 0.95,
      "sublinear_tf": true,
      "use_idf": true
    },
    "model": {
      "type": "logistic_regression",
      "params": {
        "max_iter": 1000,
        "random_state": 42,
        "C": 1.0
      }
    }
  }
}