{
  "validation": {
    "accuracy": 0.7142222222222222,
    "precision_macro": 0.48892872101102114,
    "recall_macro": 0.47383284409378046,
    "f1_macro": 0.4778315526864523,
    "precision_weighted": 0.700746787158972,
    "recall_weighted": 0.7142222222222222,
    "f1_weighted": 0.7061954733544737,
    "confusion_matrix": [
      [
        375,
        44,
        35,
        27,
        74
      ],
      [
        84,
        45,
        26,
        34,
        41
      ],
      [
        59,
        27,
        74,
        55,
        83
      ],
      [
        26,
        20,
        39,
        199,
        244
      ],
      [
        85,
        26,
        40,
        217,
        2521
      ]
    ],
    "classification_report": {
      "1": {
        "precision": 0.5961844197138315,
        "recall": 0.6756756756756757,
        "f1-score": 0.6334459459459459,
        "support": 555.0
      },
      "2": {
        "precision": 0.2777777777777778,
        "recall": 0.1956521739130435,
        "f1-score": 0.22959183673469388,
        "support": 230.0
      },
      "3": {
        "precision": 0.34579439252336447,
        "recall": 0.2483221476510067,
        "f1-score": 0.2890625,
        "support": 298.0
      },
      "4": {
        "precision": 0.37406015037593987,
        "recall": 0.3768939393939394,
        "f1-score": 0.3754716981132076,
        "support": 528.0
      },
      "5": {
        "precision": 0.8508268646641917,
        "recall": 0.8726202838352372,
        "f1-score": 0.8615857826384142,
        "support": 2889.0
      },
      "accuracy": 0.7142222222222222,
      "macro avg": {
        "precision": 0.48892872101102114,
        "recall": 0.47383284409378046,
        "f1-score": 0.4778315526864523,
        "support": 4500.0
      },
      "weighted avg": {
        "precision": 0.700746787158972,
        "recall": 0.7142222222222222,
        "f1-score": 0.7061954733544737,
        "support": 4500.0
      }
    }
  },
  "test": {
    "accuracy": 0.7217777777777777,
    "precision_macro": 0.4925132439246392,
    "recall_macro": 0.4706245445251495,
    "f1_macro": 0.4762820265674173,
    "precision_weighted": 0.7002379499032455,
    "recall_weighted": 0.7217777777777777,
    "f1_weighted": 0.7089306536067524,
    "confusion_matrix": [
      [
        397,
        33,
        33,
        21,
        72
      ],
      [
        86,
        46,
        37,
        22,
        39
      ],
      [
        58,
        26,
        67,
        58,
        89
      ],
      [
        31,
        11,
        50,
        172,
        264
      ],
      [
        70,
        21,
        50,
        181,
        2566
      ]
    ],
    "classification_report": {
      "1": {
        "precision": 0.618380062305296,
        "recall": 0.7140287769784173,
        "f1-score": 0.662771285475793,
        "support": 556.0
      },
      "2": {
        "precision": 0.3357664233576642,
        "recall": 0.2,
        "f1-score": 0.2506811989100817,
        "support": 230.0
      },
      "3": {
        "precision": 0.28270042194092826,
        "recall": 0.22483221476510068,
        "f1-score": 0.2504672897196262,
        "support": 298.0
      },
      "4": {
        "precision": 0.3788546255506608,
        "recall": 0.32575757575757575,
        "f1-score": 0.35030549898167007,
        "support": 528.0
      },
      "5": {
        "precision": 0.8468646864686469,
        "recall": 0.8885041551246537,
        "f1-score": 0.8671848597499155,
        "support": 2888.0
      },
      "accuracy": 0.7217777777777777,
      "macro avg": {
        "precision": 0.4925132439246392,
        "recall": 0.4706245445251495,
        "f1-score": 0.4762820265674173,
        "support": 4500.0
      },
      "weighted avg": {
        "precision": 0.7002379499032455,
        "recall": 0.7217777777777777,
        "f1-score": 0.7089306536067524,
        "support": 4500.0
      }
    }
  },
  "by_category": {
    "Pet_Supplies": {
      "accuracy": 0.7153333333333334,
      "f1_macro": 0.4563105426048385,
      "count": 1500
    },
    "Automotive": {
      "accuracy": 0.7523427041499331,
      "f1_macro": 0.46106585503785713,
      "count": 1494
    },
    "Video_Games": {
      "accuracy": 0.6978751660026561,
      "f1_macro": 0.4983558371729129,
      "count": 1506
    }
  },
  "experiment_id": "naive_bayes_20251206_150931",
  "config": {
    "preprocessing": {
      "lowercase": true,
      "remove_html": true,
      "remove_urls": true,
      "remove_numbers": true,
      "remove_punctuation": true,
      "remove_stopwords": true,
      "stemming": false,
      "lemmatization": true,
      "min_token_length": 2
    },
    "features": {
      "type": "count",
      "max_features": 10000,
      "ngram_range": [
        1,
        2
      ],
      "min_df": 2,
      "max_df": 0.95,
      "sublinear_tf": null,
      "use_idf": null
    },
    "model": {
      "type": "naive_bayes",
      "params": {
        "alpha": 1.0
      }
    }
  }
}