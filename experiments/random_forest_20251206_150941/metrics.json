{
  "validation": {
    "accuracy": 0.7097777777777777,
    "precision_macro": 0.8884474499140435,
    "recall_macro": 0.32964048269937596,
    "f1_macro": 0.36641087745323075,
    "precision_weighted": 0.7750523851706769,
    "recall_weighted": 0.7097777777777777,
    "f1_weighted": 0.6317313258468493,
    "confusion_matrix": [
      [
        206,
        0,
        0,
        0,
        349
      ],
      [
        31,
        19,
        0,
        2,
        178
      ],
      [
        18,
        0,
        23,
        0,
        257
      ],
      [
        5,
        0,
        0,
        63,
        460
      ],
      [
        6,
        0,
        0,
        0,
        2883
      ]
    ],
    "classification_report": {
      "1": {
        "precision": 0.7744360902255639,
        "recall": 0.37117117117117115,
        "f1-score": 0.5018270401948843,
        "support": 555.0
      },
      "2": {
        "precision": 1.0,
        "recall": 0.08260869565217391,
        "f1-score": 0.15261044176706828,
        "support": 230.0
      },
      "3": {
        "precision": 1.0,
        "recall": 0.07718120805369127,
        "f1-score": 0.14330218068535824,
        "support": 298.0
      },
      "4": {
        "precision": 0.9692307692307692,
        "recall": 0.11931818181818182,
        "f1-score": 0.21247892074198987,
        "support": 528.0
      },
      "5": {
        "precision": 0.6985703901138842,
        "recall": 0.9979231568016614,
        "f1-score": 0.821835803876853,
        "support": 2889.0
      },
      "accuracy": 0.7097777777777777,
      "macro avg": {
        "precision": 0.8884474499140435,
        "recall": 0.32964048269937596,
        "f1-score": 0.36641087745323075,
        "support": 4500.0
      },
      "weighted avg": {
        "precision": 0.7750523851706769,
        "recall": 0.7097777777777777,
        "f1-score": 0.6317313258468493,
        "support": 4500.0
      }
    }
  },
  "test": {
    "accuracy": 0.7004444444444444,
    "precision_macro": 0.8803831920733766,
    "recall_macro": 0.3118512961979363,
    "f1_macro": 0.337376286066997,
    "precision_weighted": 0.7663224197079384,
    "recall_weighted": 0.7004444444444444,
    "f1_weighted": 0.6158720398801506,
    "confusion_matrix": [
      [
        200,
        0,
        0,
        0,
        356
      ],
      [
        25,
        13,
        0,
        0,
        192
      ],
      [
        15,
        0,
        18,
        0,
        265
      ],
      [
        9,
        0,
        0,
        46,
        473
      ],
      [
        10,
        0,
        0,
        3,
        2875
      ]
    ],
    "classification_report": {
      "1": {
        "precision": 0.7722007722007722,
        "recall": 0.3597122302158273,
        "f1-score": 0.49079754601226994,
        "support": 556.0
      },
      "2": {
        "precision": 1.0,
        "recall": 0.05652173913043478,
        "f1-score": 0.10699588477366255,
        "support": 230.0
      },
      "3": {
        "precision": 1.0,
        "recall": 0.06040268456375839,
        "f1-score": 0.11392405063291139,
        "support": 298.0
      },
      "4": {
        "precision": 0.9387755102040817,
        "recall": 0.08712121212121213,
        "f1-score": 0.15944540727902945,
        "support": 528.0
      },
      "5": {
        "precision": 0.6909396779620284,
        "recall": 0.9954986149584487,
        "f1-score": 0.8157185416371117,
        "support": 2888.0
      },
      "accuracy": 0.7004444444444444,
      "macro avg": {
        "precision": 0.8803831920733766,
        "recall": 0.3118512961979363,
        "f1-score": 0.337376286066997,
        "support": 4500.0
      },
      "weighted avg": {
        "precision": 0.7663224197079384,
        "recall": 0.7004444444444444,
        "f1-score": 0.6158720398801506,
        "support": 4500.0
      }
    }
  },
  "by_category": {
    "Pet_Supplies": {
      "accuracy": 0.676,
      "f1_macro": 0.3201059730446313,
      "count": 1500
    },
    "Automotive": {
      "accuracy": 0.7356091030789826,
      "f1_macro": 0.3431130946521962,
      "count": 1494
    },
    "Video_Games": {
      "accuracy": 0.6899070385126163,
      "f1_macro": 0.3458201645461269,
      "count": 1506
    }
  },
  "experiment_id": "random_forest_20251206_150941",
  "config": {
    "preprocessing": {
      "lowercase": true,
      "remove_html": true,
      "remove_urls": true,
      "remove_numbers": true,
      "remove_punctuation": true,
      "remove_stopwords": true,
      "stemming": false,
      "lemmatization": true,
      "min_token_length": 2
    },
    "features": {
      "type": "tfidf",
      "max_features": 5000,
      "ngram_range": [
        1,
        2
      ],
      "min_df": 3,
      "max_df": 0.9,
      "sublinear_tf": false,
      "use_idf": true
    },
    "model": {
      "type": "random_forest",
      "params": {
        "n_estimators": 200,
        "random_state": 42,
        "n_jobs": -1,
        "max_depth": 50,
        "min_samples_split": 5
      }
    }
  }
}